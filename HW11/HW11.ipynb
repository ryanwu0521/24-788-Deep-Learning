{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `NotImplemented` parts of the code cells and write your answers in the markdown cells designated for your response to any questions asked. The tag `# AUTOGRADED` (all caps, with a space after `#`) should be at the beginning of each autograded code cell, so make sure that you do not change that. You are also not allowed to import any new package other than the ones already imported. Doing so will prevent the autograder from grading your code.\n",
    "\n",
    "For the code submission, run the last cell in the notebook to create the submission zip file. If you are working in Colab, make sure to download and then upload a copy of the completed notebook itself to its working directory to be included in the zip file. Finally, submit the zip file to Gradescope.\n",
    "\n",
    "After you finish the assignment and fill in your code and response where needed (all cells should have been run), save the notebook as a PDF using the `jupyter nbconvert --to pdf HW11.ipynb` command (via a notebook code cell or the command line directly) and submit the PDF to Gradescope under the PDF submission item. If you cannot get this to work locally, you can upload the notebook to Google Colab and create the PDF there. You can find the notebook containing the instruction for this on Canvas.\n",
    "\n",
    "If you are running the notebook locally, make sure you have created a virtual environment (using `conda` for example) and have the proper packages installed. We are working with `python=3.10` and `torch>=2`.\n",
    "\n",
    "Files to be included in submission:\n",
    "\n",
    "- `HW11.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from HW11_utils import Tracker\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    Device = 'mps'\n",
    "else:\n",
    "    Device = 'cpu'\n",
    "print(f'Device is {Device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive loss functions (40)\n",
    "\n",
    "A common loss function for contrastive learning is the NT-Xent loss introduced in [SimCLR](https://arxiv.org/pdf/2002.05709), which tries to maximize the cosine similarity between the features of the augmented data pairs that come from the same sample. The loss is a normalized temperature-scaled cross-entropy, i.e. NT-Xent, and is defined as follows for each pair:\n",
    "$$\n",
    "\\ell_{i,j} = - \\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k) / \\tau)}\n",
    "$$\n",
    "The final loss is the average of the loss _over the positive pairs_, as you can also see in the pseudo-code in the paper. You have to implement this loss function as a module, and use this layer in your contrastive pre-training. Make sure the devices of the tensors are consistent.\n",
    "\n",
    "The next loss that you are going to implement is the the [Barlow Twins Loss](https://arxiv.org/pdf/2103.03230) with the aim to reduce feature redundancy. With the SimCLR loss funcion, the model could optimize the loss by simply filling the whole feature vector with just one feature that is the same across all feature dimensions, and that would hypothetically minimize the loss function. However, we would want a rich representation rather than one feature or a redundant feature vector with a lot of repetitive features. The Barlow Twins loss works with the correlation of the features with each other in a batch to make sure that the features are not redundant. For your implementation, you can refer to the pseudo-code in the paper.\n",
    "\n",
    "NOTE: You can and should implement these loss functions without any `for` loops. Each for loop will have a penalty of -5. You may find `torch.diag` and `torch.eye` helpful. For numerical stability, we suggest using `F.log_softmax` or `F.cross_entropy` for `NT_Xent` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGRADED\n",
    "\n",
    "class NT_Xent(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            temp: float = 0.5,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            z1: torch.FloatTensor, # (N, D)\n",
    "            z2: torch.FloatTensor, # (N, D)\n",
    "            ):\n",
    "        \"\"\"\n",
    "        z1 and z2 are two augmented batches of the same data.\n",
    "        i.e. z1[i] and z2[i] are augmentations of the same image for each i.\n",
    "        \"\"\"\n",
    "        NotImplemented\n",
    "\n",
    "\n",
    "class Barlow_Twins(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lambda_: float = 5e-3,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            z1: torch.FloatTensor, # (N, D)\n",
    "            z2: torch.FloatTensor, # (N, D)\n",
    "            ) -> torch.FloatTensor: # ()\n",
    "        \n",
    "        NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW11_utils import Test_NT_Xent, Test_Barlow_Twins\n",
    "\n",
    "Test_NT_Xent(NT_Xent)\n",
    "Test_Barlow_Twins(Barlow_Twins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Pre-training with the CIFAR-10 dataset (60)\n",
    "\n",
    "Now you can use your loss functions to pre-train a ResNet using contrastive self-supervised learning. First, download the dataset using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(\n",
    "    root = './CIFAR10',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    "    )\n",
    "\n",
    "test_dataset = CIFAR10(\n",
    "    root = './CIFAR10',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    "    )\n",
    "\n",
    "print(f'train dataset size, {len(train_dataset)}')\n",
    "print(f'test dataset size, {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation (10)\n",
    "\n",
    "Implement the augmentation class to be used for generating data pairs in contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size = 32,\n",
    "        ):\n",
    "    \n",
    "        color_jitter = T.ColorJitter(\n",
    "            brightness = 0.4,\n",
    "            contrast = 0.4,\n",
    "            saturation = 0.4,\n",
    "            hue = 0.1,\n",
    "            )\n",
    "        \"\"\"\n",
    "        apply the following augmentations:\n",
    "        Random resized crop to img_size\n",
    "        Random horizontal flip with 50% probability\n",
    "        Random color jitter with 80% probability\n",
    "        Random grayscale with 20% probability\n",
    "        \"\"\"\n",
    "        self.transform = T.Compose([\n",
    "            NotImplemented\n",
    "        ])\n",
    "    \n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation functions (10)\n",
    "\n",
    "Fill in `train_epoch` to complete the function for the case of contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    trainloader: DataLoader,\n",
    "    augment: Union[None, Augment],\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = Device,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    A flexible function for a training epoch.\n",
    "    Performs supervised learning if augment is None.\n",
    "    Performs contrastive self-supervised learning if augment is not None.\n",
    "    returns the losses for all training batches over the epoch as a list.\n",
    "    \"\"\"\n",
    "    model.train().to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for x, y in trainloader:\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if augment is None:\n",
    "            # supervised learning\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "        else:\n",
    "            # self-supervised contrastive learning\n",
    "            NotImplemented\n",
    "            loss = NotImplemented\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        assert not torch.isnan(loss)\n",
    "        assert not torch.isinf(loss)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
    "    device: str = Device,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataloader.\n",
    "    Only used for supervised learning of multi-class classification.\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "\n",
    "    loss_sum = 0.\n",
    "    acc_sum = 0.\n",
    "    N = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        N += len(x)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        assert not torch.isnan(loss)\n",
    "        assert not torch.isinf(loss)\n",
    "\n",
    "        loss_sum += loss.item()*len(x)\n",
    "        acc_sum += (y_pred.argmax(-1) == y).float().sum().item()\n",
    "\n",
    "    return loss_sum / N, acc_sum / N\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataset: Dataset,\n",
    "    test_dataset: Union[None, Dataset], # pass None for SSL\n",
    "    augment: Union[None, Augment], # pass None for supervised learning\n",
    "    loss_fn: nn.Module,\n",
    "    device: str = Device,\n",
    "    plot_freq: int = 1,\n",
    "\n",
    "    optim_name: str = 'Adam', # from optim\n",
    "    optim_config: dict = dict(),\n",
    "    lr_scheduler_name: Union[str, None] = None, # from optim.lr_scheduler\n",
    "    lr_scheduler_config: dict = dict(),\n",
    "\n",
    "    n_epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    ):\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "\n",
    "    if augment is None: # supervised learning\n",
    "        evalloader_traindata = DataLoader(train_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            evalloader_testdata = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    optimizer: optim.Optimizer = optim.__getattribute__(optim_name)(model.parameters(), **optim_config)\n",
    "    if lr_scheduler_name is not None:\n",
    "        lr_scheduler: optim.lr_scheduler._LRScheduler = optim.lr_scheduler.__getattribute__(lr_scheduler_name)(optimizer, **lr_scheduler_config)\n",
    "\n",
    "    epoch_pbar = tqdm(\n",
    "        range(1, n_epochs+1),\n",
    "        desc = 'epochs',\n",
    "        unit = 'epoch',\n",
    "        dynamic_ncols = True,\n",
    "        leave = True,\n",
    "        )\n",
    "    \n",
    "    tracker = Tracker(n_epochs = n_epochs, plot_freq = plot_freq)\n",
    "    training_loss = None\n",
    "    # evaluation metrics:\n",
    "    train_loss, train_acc, test_loss, test_acc = 4*[None]\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "\n",
    "        losses = train_epoch(\n",
    "            model = model,\n",
    "            trainloader = trainloader,\n",
    "            augment = augment,\n",
    "            loss_fn = loss_fn,\n",
    "            optimizer = optimizer,\n",
    "            device = device,\n",
    "            )\n",
    "        training_loss = np.mean(losses)\n",
    "\n",
    "        if lr_scheduler_name == 'ReduceLROnPlateau':\n",
    "            lr_scheduler.step(training_loss)\n",
    "        elif lr_scheduler_name is not None:\n",
    "            lr_scheduler.step()\n",
    "        postfix_str = f'training loss: {training_loss:.4f}'\n",
    "        \n",
    "        # supervised learning evaluation\n",
    "        if augment is None:\n",
    "\n",
    "            train_loss, train_acc = eval_epoch(\n",
    "                model = model,\n",
    "                dataloader = evalloader_traindata,\n",
    "                loss_fn = loss_fn,\n",
    "                device = device,\n",
    "                )\n",
    "            postfix_str += f', train loss: {train_loss:.4f}, train acc: {train_acc:.4f}'\n",
    "\n",
    "            if test_dataset is not None:    \n",
    "                test_loss, test_acc = eval_epoch(\n",
    "                    model = model,\n",
    "                    dataloader = evalloader_testdata,\n",
    "                    loss_fn = loss_fn,\n",
    "                    device = device,\n",
    "                    )\n",
    "                postfix_str += f', test loss: {test_loss:.4f}, test acc: {test_acc:.4f}'\n",
    "\n",
    "        epoch_pbar.set_postfix_str(postfix_str)\n",
    "        tracker.update(training_loss, train_loss, train_acc, test_loss, test_acc)\n",
    "\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive pre-trining (10)\n",
    "\n",
    "Now, we use the pre-defined `resnet18` with some modification for CIFAR10 dataset. Your task is to replace the projection head with a 2-layer feedforward network. This will be the $g$ in the SimCLR paper, and the $f$ is everything before that. After contrastive learning, we will discard $g$ and keep $f$ as a feature extractor. Then. we will train a linear classifier that uses the features from $f$. The tracker will only show one learning curve for the contrastive training loss in this section. The pretraining might take a while, and the rate of covergence is relatively slow compared to supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a resnet18 model and modifying it for CIFAR10 according to SimCLR paper.\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(\n",
    "    in_channels = 3, \n",
    "    out_channels = 64, \n",
    "    kernel_size = 3, \n",
    "    stride = 1, \n",
    "    padding = 1, \n",
    "    bias = False,\n",
    "    )\n",
    "model.maxpool = nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the code for attaching a projection head $g$ and pretraining the model. To verify the effect of pretraining, skip this cell in your first try and complete the next sections (feature extraction and linear classification) using the model that has not been pretrained. Remember the results. Then try to complete the next sections after pretraining. If everything is correct, your classification result should improve by pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now replace the final layer of resnet with a 2-layer MLP using nn.Sequential(). This is the projection head g.\n",
    "# The MLP has hidden size 2048 and output size 128. set bias=False, and use ReLU activation.\n",
    "NotImplemented\n",
    "\n",
    "\n",
    "loss_fn = NT_Xent(temp = 0.5) # DO NOT CHANGE!\n",
    "augment = Augment()\n",
    "\n",
    "# SSL training configuration:\n",
    "ssl_config = dict(\n",
    "    optim_name = 'Adam',\n",
    "    optim_config = dict(lr=1e-3),\n",
    "    lr_scheduler_name = 'ReduceLROnPlateau',\n",
    "    lr_scheduler_config = dict(factor=0.1, patience=2),\n",
    "    n_epochs = 20,\n",
    "    batch_size = 256, # Bigger batch sizes are better for contrastive learning \n",
    "    )\n",
    "\n",
    "# Run the SSL training\n",
    "tracker = train(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = None,\n",
    "    loss_fn = loss_fn,\n",
    "    augment = augment,\n",
    "    device = Device,\n",
    "    plot_freq = 1,\n",
    "    **ssl_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using the pre-trained feature extractor $f$ (10)\n",
    "\n",
    "Discard the projection head by replacing it with an Identity module. Then, extract the features using the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarding the projection head (g in SimCLR paper)\n",
    "NotImplemented\n",
    "\n",
    "# Passing the whole dataset through the feature extractor to get the feature representations\n",
    "@torch.inference_mode()\n",
    "def get_dataset_features(\n",
    "    model: nn.Module, \n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 256,\n",
    "    device: str = Device,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    A helper function to process the dataset for finetuning.\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # extract the features and append them to the list\n",
    "        features.append(NotImplemented)\n",
    "\n",
    "        labels.append(y)\n",
    "\n",
    "    features = torch.cat(features, dim=0).cpu()\n",
    "    labels = torch.cat(labels, dim=0).cpu()\n",
    "\n",
    "    return TensorDataset(features, labels)\n",
    "\n",
    "# Get the featurized train and test dataset using the function above\n",
    "finetune_train_dataset_features = NotImplemented\n",
    "finetune_test_dataset_features = NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning with a linear classifier (10)\n",
    "\n",
    "Define a linear classifier with no bias and train it on the featurized datasets. You should get around 60% test accuracy. For contrastive pre-training, a very large batch size and a very long training is usally needed for great results, which is not in the budget of a homework. However, you have learned the pipeline in this assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NotImplemented\n",
    "\n",
    "train_config = dict(\n",
    "        optim_name = 'Adam',\n",
    "        optim_config = dict(lr = 1e-3),\n",
    "        lr_scheduler_name = 'ReduceLROnPlateau',\n",
    "        lr_scheduler_config = dict(factor = 0.5, patience = 3),\n",
    "        n_epochs = 30,\n",
    "        batch_size = 64,\n",
    "        )\n",
    "\n",
    "# YOUR CODE\n",
    "tracker = train(\n",
    "    model = NotImplemented,\n",
    "    train_dataset = NotImplemented,\n",
    "    test_dataset = NotImplemented,\n",
    "    augment = None,\n",
    "    loss_fn = nn.CrossEntropyLoss(),\n",
    "    device = Device,\n",
    "    plot_freq = 1,\n",
    "    **train_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together as a full classifier (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final model by combining the feature extractor and the classifier.\n",
    "# (one line of code should be enough)\n",
    "final_model = NotImplemented\n",
    "\n",
    "# calculating the loss and accuracy on the train and test dataset\n",
    "\n",
    "train_loss, train_acc = eval_epoch(\n",
    "    model = final_model,\n",
    "    dataloader = DataLoader(train_dataset, batch_size=64),\n",
    "    device = Device,\n",
    "    )\n",
    "\n",
    "test_loss, test_acc = eval_epoch(\n",
    "    model = final_model,\n",
    "    dataloader = DataLoader(test_dataset, batch_size=64),\n",
    "    device = Device,\n",
    "    )\n",
    "\n",
    "# You should get the same thing as the last epoch of your supervised learning!\n",
    "print(f'train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
    "print(f'test loss: {test_loss:.4f}, test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip files for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW11_utils import zip_files\n",
    "\n",
    "submission_files = ['HW11.ipynb']\n",
    "zip_files('HW11_submission.zip', submission_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
